---
title: "Simulating Stroop effects with ChatGPT"
author: "Matt Crump"
date: 6/6/23
description: "This is a short post to determine how easy it would be to simulate performance in a classic human attention task using ChatGPT. The short answer is that it can generate data in the style of a Stroop experiment without too much effort on my part."
bibliography: references.bib
---

I had a couple of hours to spare this afternoon, so I tried to simulate performance in a Stroop task using ChatGPT and the [openai](openai.com) API. This post will be brief, and I'm mainly using the time to see if I can get the api to work. Let's go.

In a Stroop task [for a review see, @macleodHalfCenturyResearch1991] participants are asked to respond quickly and accurately to identify stimuli with target and distractor dimensions. In the examples below, each stimulus is a word printed in a color. The target dimension is the ink-color, and the distractor dimension is the word name. The task is to identify the ink-color as quickly and accurately as possible.

```{r}
#| echo: false
knitr::include_graphics("Stroop.png")
```

There are congruent and incongruent item types. Congruent items have matching color and word dimensions (the word BLUE written in blue ink). Incongruent items have mismatching color and word dimensions (the word BLUE written in red ink). The Stroop effect is the finding that responses are usually faster and more accurate for congruent than incongruent items.

## Who cares if ChatGPT can simulate the Stroop effect?

I'm not sure. ChatGPT is a large language model, and although I have some familiarity with how I think it works, I don't know most of the important details...especially compared to the level of detail I'm used to from computational models in cognitive psychology. For this reason, I'm not interested in modeling Stroop performance with ChatGPT for any explanatory purpose.

Based on messing around with GPT models previously, I'm pretty confident that some series of prompts will produce data in the style of a Stroop experiment.

Why do I care about this? Some of my research involves questions about attentional control abilities [@bugg2012support; @braem2019measuring], and some of my work uses online methods to collect data. For example, experimental tasks are programmed for web-browsers using tools like JsPsych [@de2015jspsych], and participants may be recruited from platforms like Amazon's Mechanical Turk [@crump2013evaluating]. Online methods are useful research tools, but they are not without problems.

One of my concerns as a cognitive psychologist interested in human behavior is that participants are actual people completing tasks to the best of their understanding. I'm usually not interested in analyzing data spoofed by a bot. 

I haven't encountered major issues with bots in the tasks I run. These tasks require quick responses to recognize words and letters. Although it is possible to develop a bot to perform these actions, it wouldn't be profitable for the bot programmer.

In the last few months I've run a tasks on Amazon Mechanical Turk and opened them up to a general audience with almost no restrictions. This produced very low quality data and reminded me of several topics discussed in Todd Gureckis' [mturk blog post from a couple years ago](https://todd.gureckislab.org/2021/06/09/the-poisoned-well). In the section on a nightmare world of human-bot hybrids Todd mentioned commonly available browser plugins that record a person's interactions with a website and then replay them back. In an online psych study, this could involve completing the task one time (with all of the responses being recorded by the plugin), and then reloading the study under a different user name and completing it again using the previously recorded responses. It seemed to me that some mturk workers were doing this (or similar).

During task debriefing sessions, I use various questions to inquire about participants' experiences. However, I have recently observed that the responses from some mturk workers resemble those generated by ChatGPT. Notably, I found fewer issues like this when using more restrictive qualifications, such as master worker, over 95% HIT completion rate, and over 200 HITS completed.

In the most recent cases where I suspected the data was compromised it was also really easy to tell. For example, accuracy was at chance or, reaction times were way too fast. The strategy of using a browser plugin to record and then replay previous responses doesn't produce above chance performance when the task randomizes the stimuli when ever the web-page is reloaded.

In any case, my question this afternoon was to do a bit of detective work to determine how easy it would be to spoof some Stroop data using ChatGPT. I'm not going to write a browser plugin to automate a GPT-bot capable of spoofing data in the context of browser-based task (say written using JsPsych, or Lab.js, or similar). But, I'm going to test a few prompts and see what kind of fake accuracy and reaction time data will be produced.

## How I'm doing this

I'm running out of time, so the short version is:

1.  Get an openai.com account
2.  Install the openai R library <https://github.com/irudnyts/openai>
3.  Get an api key, and do the appropriate stuff
4.  Follow along with the minimally commented code below

### Load the libraries

```{r}
#| message: false
#| warning: false
library(openai)
library(tidyverse)
```

### Test One Simulated Subject

I'll use the gpt-3.5-turbo model. I'm creating a list of 24 Stroop trials, with 12 congruent and 12 incongruent items. The list is then randomized.

I'm using the chat completion api, with the system prompt:

------------------------------------------------------------------------

You are a simulated participant in a human cognition experiment. Your task is to respond as quickly and accurately as possible, and record your simulated responses in a JSON file

------------------------------------------------------------------------

Then, to request fake data for a Stroop experiment I write something like:

------------------------------------------------------------------------

Consider the following trials of a Stroop task where you are supposed to identify the ink-color of the word as quickly and accurately as possible.

\-\-\-\--

1 The word yellow printed in the color green

2 The word red printed in the color yellow

3 The word blue printed in the color blue

4 The word green printed in the color yellow

5 The word blue printed in the color blue

6 The word blue printed in the color green

7 The word green printed in the color green

and so on for 24 trials (note the code specifies all 24 trials in the prompt)

\-\-\-\--

Put simulated identification responses and reaction times into a JSON with keys 'trial', 'word', 'color', 'response', 'reaction_time':

------------------------------------------------------------------------

Here's the code:

```{r, eval = FALSE}
# use the colors red, green, blue, and yellow

# four possible congruent items
congruent_items <- c("The word red printed in the color red",
                     "The word blue printed in the color blue",
                     "The word yellow printed in the color yellow",
                     "The word green printed in the color green")

# four possible incongruent items
incongruent_items <- c("The word red printed in the color blue",
                       "The word red printed in the color green",
                       "The word red printed in the color yellow",
                       "The word blue printed in the color red",
                       "The word blue printed in the color green",
                       "The word blue printed in the color yellow",
                       "The word yellow printed in the color red",
                       "The word yellow printed in the color blue",
                       "The word yellow printed in the color green",
                       "The word green printed in the color red",
                       "The word green printed in the color blue",
                       "The word green printed in the color yellow")

# generate 50% congruent and 50% incongruent trials
# 12 each (congruent and incongruent)
trials <- sample(c(rep(congruent_items,3),incongruent_items))

# submit a query to open ai using the following prompt
# note: responses in JSON format are requested
gpt_response <- create_chat_completion(
   model = "gpt-3.5-turbo",
   messages = list(
       list(
           "role" = "system",
           "content" = "You are a simulated participant in a human cognition experiment. Your task is to respond as quickly and accurately as possible, and record your simulated responses in a JSON file"),
       list("role" = "user",
           "content" = paste("Consider the following trials of a Stroop task where you are supposed to identify the ink-color of the word as quickly and accurately as possible.","-----", paste(1:24, trials, collapse="\n") , "-----","Put simulated identification responses and reaction times into a JSON with keys 'trial', 'word', 'color', 'response', 'reaction_time':", sep="\n")
       )
   )
)


# model responses are in JSON format
sim_data <- jsonlite::fromJSON(gpt_response$choices$message.content)
save.image("batch1.RData")
```

## Results

It is convenient that telling ChatGPT to return data in a JSON format actually works. Converting to a tibble, we can print out the trial-by-trial response from the model.

### The "raw" data

Here is the JSON returned by ChatGPT. It has filled out the response column for each trial, and it has provided a reaction time value for each trial.

To be more accurate, it has returned this whole JSON file and written all of the columns. I still need to check whether the trial-to-trial data in this output file is the same as the order in the prompt.

These numbers look plausible-ish...although they all end in 0, which isn't very plausible.

```{r}
load(file = "batch1.RData")
knitr::kable(sim_data)
```

### Mean Reaction times

And, now the moment I've been waiting for...Is there a Stroop effect? To answer this question I computed the mean reaction time for congruent and incongruent items. The answer is that YES, ChatGPT produces reaction time data in the style of Stroop experiment that does result in faster mean reaction times for congruent than incongruent items.

```{r}

# report rt data
rt_data <- sim_data %>%
  mutate(congruency = case_when(word == color ~ "congruent",
                                word != color ~ "incongruent")) %>%
  group_by(congruency) %>%
  summarize(mean_rt = mean(reaction_time))

knitr::kable(rt_data)
```

### Accuracy Data

This time, ChatGPT didn't make any mistakes.

```{r}
# report accuracy data
accuracy_data <- sim_data %>%
  mutate(congruency = case_when(word == color ~ "congruent",
                                word != color ~ "incongruent")) %>%
  mutate(accuracy = case_when(response == color ~ TRUE,
                              response != color ~ FALSE)) %>%
  group_by(congruency) %>%
  summarize(accuracy = sum(accuracy)/12)

knitr::kable(accuracy_data)
```

## Next Steps

I've run out of time to work more on this right now, but I will add more to this post another day.

A next step will be to simulate data for many different ChatGPT participants. I'm curious if the spoofed RT distributions will have human-like qualities (e.g., ex-guassian). I'm also interested in whether slight variations to the instructions (e.g., complete this task with about 75% accuracy, or complete this task as if you were tired) systematically influence the results.

So far my main conclusion is that it wouldn't be that hard to hack my studies using this kind of approach. Sigh.

## Multiple simulated subjects

I tried to run the following, which loops the above 10 times. I ran into an error on the third iteration because the JSON was not formatted correctly. Added some validation for the JSON. The JSON was not consistent, so I changed the prompt up a bit.

So far I haven't been able to run this loop. I got through 7 iterations, and then received a message that the server was overloaded. Also, only 2 of those 7 iterations produced valid JSON. I'm going to put this on the back burner for now.

```{r, eval = FALSE}
# use the colors red, green, blue, and yellow

# four possible congruent items
congruent_items <- c("The word red printed in the color red",
                     "The word blue printed in the color blue",
                     "The word yellow printed in the color yellow",
                     "The word green printed in the color green")

# four possible incongruent items
incongruent_items <- c("The word red printed in the color blue",
                       "The word red printed in the color green",
                       "The word red printed in the color yellow",
                       "The word blue printed in the color red",
                       "The word blue printed in the color green",
                       "The word blue printed in the color yellow",
                       "The word yellow printed in the color red",
                       "The word yellow printed in the color blue",
                       "The word yellow printed in the color green",
                       "The word green printed in the color red",
                       "The word green printed in the color blue",
                       "The word green printed in the color yellow")

# generate 50% congruent and 50% incongruent trials
# 12 each (congruent and incongruent)
trials <- sample(c(rep(congruent_items,3),incongruent_items))


#set up variables to store data
all_sim_data <- tibble()
gpt_response_list <- list()

# request multiple subjects
# submit a query to open ai using the following prompt
# note: responses in JSON format are requested

for(i in 1:10){
  print(i)
  
  gpt_response <- create_chat_completion(
   model = "gpt-3.5-turbo",
   messages = list(
       list(
           "role" = "system",
           "content" = "You are a simulated participant in a human cognition experiment. Your task is to respond as quickly and accurately as possible, and record your simulated responses in a JSON file"),
       list("role" = "user",
           "content" = paste("Consider the following trials of a Stroop task where you are supposed to identify the ink-color of the word as quickly and accurately as possible.","-----", paste(1:24, trials, collapse="\n") , "-----","Put simulated identification responses and reaction times into a JSON array of objects following this format: [{'trial': 'trial number', 'word': 'the name of the word, string','color': 'the color of the word, string','response': 'the simulated identification response, string','reaction_time': 'the simulated reaction time, milliseconds an integer'}]", sep="\n")
           
       )
   )
)
  
  # save the output from openai
  gpt_response_list[[i]] <- gpt_response
  
  # validate the JSON  
  test_JSON <- jsonlite::validate(gpt_response$choices$message.content)
  
  # validation checks pass, write the simulated data to all_sim_data 
  if(test_JSON == TRUE){
    sim_data <- jsonlite::fromJSON(gpt_response$choices$message.content)
    
    if(sum(names(sim_data) == c("trial","word","color","response","reaction_time")) == 5) {
      sim_data <- sim_data %>%
        mutate(sim_subject = i)
  
      all_sim_data <- rbind(all_sim_data,sim_data)
    }
    
  }
}

# model responses are in JSON format
#save.image("batch2.RData")
```




