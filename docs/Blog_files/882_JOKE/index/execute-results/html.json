{
  "hash": "07fa7a08d3160f849b16c361a9243326",
  "result": {
    "markdown": "---\ntitle: \"Archiving the journal of knowledge\"\ndescription: \"Attempt to download some previous tweets before deleting an account.\"\nauthor: \"Matt Crump\"\ndate: '2022-10-28'\ndate-modified: last-modified\nimage: 'images/Journal_of_knowledge.png'\nopengraph:\n  twitter:\n    card: summary_large_image\n    creator: \"@MattCrumpLab\"\n    site: \"@MattCrumpLab\"\ncategories:\n  - social\n  - twitter\n  - parody\n---\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/Journal_of_knowledge.png){width=100%}\n:::\n:::\n\n\nSome time ago I made a journal parody account on twitter called \"The Journal of Knowledge\" (circa 2018). The account has been inactive for years, and I'm deleting it.\n\nIn preparation to delete the account I made a data request to twitter, but I'm not sure what data I get before I close the account. I recently got the `rtweet` package working, so I'm going to find out if I can use it to download the old posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rtweet)\n\nauth <- rtweet_app()\n\nJOKE_timeline <- rtweet::get_timeline(user=\"journal_O_K\",\n                              n = 200,\n                              token = auth)\n\nsaveRDS(JOKE_timeline,\"journal_of_knowledge.RDS\")\n```\n:::\n\n\nThat pulled all 173 tweets from the account into a data frame, and I saved it as an .RDS file so I can load it later. But, this did not download any of the pictures of the fake journal abstracts, and that is what I want to archive.\n\nThere's a function for screenshotting twitter posts, maybe I can use this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJOKE_timeline$id_str[3]\nmgk_img <- tweet_shot(JOKE_timeline$id_str[3], zoom = 3, scale = TRUE)\nmagick::image_write(mgk_img,\"test.png\")\n```\n:::\n\n\nNo, that function was deprecated.\n\nNeed to roll my own.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get urls for images to download\nmedia_urls <- c()\n\nfor(i in 1:173){\n  media_urls[i] <- JOKE_timeline$entities[[i]]$media$media_url\n}\n\nmedia_urls <- media_urls[is.na(media_urls) == FALSE]\n\n# download all the images into folder\n\n?download.file\n\nfor (i in 2:length(media_urls)){\n  f_name <- tail(unlist(strsplit(media_urls[i],\"/\")),1)\n  f_path <- paste0(\"images/\",f_name)\n  \n  download.file(media_urls[i],f_path)\n}\n```\n:::\n\n\nI think I got what I wanted. Now, I just need to delete some stuff, and then maybe share a few Journal of Knowledge abstracts for posterity.\n\n## JOKE abstracts\n\nThere were 68 abstracts posted on the account, and I was able to download all of them. Here's a few of them.\n\n### What is the answer to this question? It depends\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/DgTg1uVX4AA-V_W.jpg){width=536}\n:::\n:::\n\n\n### Blah blah is special: No it isn't\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/DgueZsEWsAAZBWK.jpg){width=543}\n:::\n:::\n\n\n### The population is aging\n\nTo the tune of...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/Dgx8n8_W0AEKEdU.jpg){width=551}\n:::\n:::\n\n\n### Journal of Feelings\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/Dh_axdNUEAE-iaE.jpg){width=538}\n:::\n:::\n\n\n### Dressing up research results\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](images/DhgMm9KU0AE9LJZ.jpg){width=536}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}