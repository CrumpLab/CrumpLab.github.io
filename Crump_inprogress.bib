

@article{BrosowskyCrump2019CJEP,
  title = {Contextual recruitment of selective attention can be updated via changes in task-relevance},
  abstract = {Evidence across a wide variety of attention paradigms shows that environmental cues can trigger adjustments to ongoing priorities for attending to relevant and irrelevant information. This context-specific control over attention suggests that cognitive control can be both automatic and flexible. For instance, in selective attention tasks, congruency effects are larger for items that appear in a context associated with infrequent conflict than in a context associated with frequent conflict. Since the to-be-presented context cannot be predicted or prepared for in advance, attention is assumed to be rapidly updated on-the-fly, triggered by the currently presented context. Context-specific control exemplifies how learning and memory processes can influence attention to enable cognitive flexibility. However, what determines the use of previously learned associations still remains unclear. In the current study, we examined whether task-relevance would influence the learning and use of context cues in a flanker task. Using a secondary counting task, context dimensions associated with differing levels of conflict were made task-relevant or-irrelevant across the experiment. In short, we found that making new contextual information task-relevant caused participants to ignore a previously learned context-attention association and adopt a new context-specific control strategy; all without changing the experimental stimuli. This result suggests that task-relevance is a key determinant of context-specific control.},
  author = {Brosowsky, Nicholaus P. and Crump, Matthew J. C.},
  journal = {Under review at Canadian Journal of Psychology},
  year = {202?},
  doi = {10.31234/osf.io/43tj7},
  volume = {preprint},
  publisher = {PsyArXiv},
  pages = {no pages yet},
  url_Preprint = {https://psyarxiv.com/43tj7/},
  url_Data = {https://osf.io/ztcyb/}
}


@article{JohnsetalJML,
  title = {Production without Rules: Using an Instance Memory Model to Exploit Structure in Natural Language},
  abstract = {Recent research in the artificial grammar learning has shown that a simple instance model of memory can account for a wide variety of artificial grammar results (Jamieson & Mewhort, 2009, 2010, 2011), indicating that language processing may have more in common with episodic memory than previously thought. These results have been used to develop new instance models of natural language processing, including a model of sentence comprehension (Johns & Jones, 2015) and semantic memory (Jamieson, Avery, Johns, & Jones, 2018). The foundations of the models lie in the storage and retrieval of episodic traces of linguistic experience. The current research extends the idea to account for natural language sentence production. We show that the structure of language itself provides sufficient information to generate syntactically correct sentences, even with no higher-level information (such as knowledge of a grammar) available to the model. This work provides insight into the highly structured nature of natural language, and how instance memory models can be a powerful model type to exploit this structure. Additionally, it demonstrates the utility of using the formalisms developed in episodic memory research to understand performance in other domains, such as in language processing.},
  author = {Johns, Brendan T. and Jamieson, Randall K. and Crump, Matthew J. C. and Jones, Michael N. and Mewhort D. J. K.},
  journal = {Journal of Memory and Language},
  year = {Under Review}
}

